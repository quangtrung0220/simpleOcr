{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simpleOcr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quangtrung0220/simpleOcr/blob/main/simpleOcr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHqdaDyviNIb"
      },
      "source": [
        "### ***Import các thư viện cần thiết***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOR4tIyWxCtG"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS5NUyqriqjn"
      },
      "source": [
        "### ***Clone git để lấy dữ liệu ảnh train và test***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVMBQrJ-TkpR",
        "outputId": "1982df62-ef3d-407f-983f-05bf06bfe418"
      },
      "source": [
        "# Mount với drive để lấy datasets \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Link git lấy dataset \n",
        "# !git clone https://github.com/quangtrung0220/SieuAmTim.git\n",
        "# traindir = \"/content/SieuAmTim/DATA_CHAMBER_2021/train\"\n",
        "# testdir = \"/content/SieuAmTim/DATA_CHAMBER_2021/test\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oBKf_SGRmSV"
      },
      "source": [
        "traindir = \"/content/drive/MyDrive/data\"\n",
        "testdir = \"/content/drive/MyDrive/data_test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGn9YjaGi51f"
      },
      "source": [
        "### ***Lấy ra các nhãn - classes và chuẩn bị dữ liệu***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Các nhãn: HighLands, StarBucks, Others, PhucLong\n",
        "*   KKích cỡ ảnh train và test: 224x224\n",
        "*   Preprocess: Cân bằng sáng: histogram, Lọc nhiễu: GaussianBlur\n",
        "*   Data Augmentation: RandomCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
        "*   Đưa vào batch để tận dụng khả năng xử lý song song của GPU\n",
        "*   Folder train và test lần lượt trong: traindir và testdir\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Ekbuc8UWLr"
      },
      "source": [
        "# Hàm lấy ra các classes\n",
        "def get_clases():\n",
        "  classes = ['HighLands', 'Other', 'PhucLong', 'StarBuck']\n",
        "  return classes\n",
        "\n",
        "TrainTest = namedtuple('TrainTest', ['train', 'test'])\n",
        "\n",
        "# Hàm chuẩn bị dữ liệu\n",
        "def prepare_data():\n",
        "\n",
        "  #Augmentation for all\n",
        "  horizontal = transforms.RandomHorizontalFlip()\n",
        "  vertical = transforms.RandomVerticalFlip()\n",
        "\n",
        "  #Image 224x224\n",
        "  resize_224 = transforms.Resize((224,224))\n",
        "  crop_224 = transforms.RandomCrop(224, padding=4)\n",
        "\n",
        "  #To tensor\n",
        "  tensor = transforms.ToTensor()\n",
        "\n",
        "  #Raw image\n",
        "  transform_train_raw224 = transforms.Compose([resize_224, tensor])\n",
        "\n",
        "  #Augmentation image\n",
        "  transform_train_aug224 = transforms.Compose([resize_224, crop_224, horizontal, vertical, tensor])\n",
        "\n",
        "  #Test image\n",
        "  transform_test_224 = transforms.Compose([resize_224, tensor])\n",
        "\n",
        "  trainset = torchvision.datasets.ImageFolder(root=traindir, transform=transform_train_raw224)\n",
        "  testset = torchvision.datasets.ImageFolder(root=testdir, transform=transform_test_224)\n",
        "  return TrainTest(train=trainset, test=testset)\n",
        "\n",
        "# Hàm load dữ liệu\n",
        "def prepare_loader(datasets):\n",
        "  trainloader = DataLoader(dataset=datasets.train, batch_size=8, shuffle=True, num_workers=4)\n",
        "  testloader = DataLoader(dataset=datasets.test, batch_size=8, shuffle=False, num_workers=4)\n",
        "  return TrainTest(train=trainloader, test=testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYQS8gvZqB6F"
      },
      "source": [
        "### ***Hàm train và test***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   BatchSize = 8\n",
        "*   Report sau mỗi 9 epoch \n",
        "*   Train: cho đi qua model và tính lỗi bằng loss_func sau đó cập nhật tham số mới\n",
        "*   Test: khai báo chế độ eval(đánh giá - evaluate) và trả về nhãn dự đoán và nhãn thực tế\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5aaQjZ6q7tM"
      },
      "source": [
        "# Data = prepare_data()\n",
        "# img, label = Data.train[3000]\n",
        "# print(label)\n",
        "# plt.show(img)\n",
        "# print(img)\n",
        "# s = Data.train.imgs[0]\n",
        "# print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBdgazuRRB3K"
      },
      "source": [
        "# Hàm train \n",
        "def train_epoch(epoch, model, loader, loss_func, optimizer, device):\n",
        "  true_result = []\n",
        "  pred_result = []\n",
        "\n",
        "  # gọi hàm train để model biết mình phải cần huấn luyện \n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  reporting_steps = 9\n",
        "  for i, (images, labels) in enumerate(loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = model(images)\n",
        "    true_result += list(labels.cpu().numpy())\n",
        "    _, predicted = torch.max(outputs, dim=1)\n",
        "    pred_result += list(predicted.cpu().numpy())\n",
        "    # Tính lỗi \n",
        "    loss = loss_func(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % reporting_steps == reporting_steps-1:\n",
        "      print(f\"Epoch {epoch} step {i} ave_loss {running_loss/reporting_steps:.4f}\")\n",
        "      running_loss = 0.0\n",
        "  # return pred_result, true_result\n",
        "\n",
        "# hàm test \n",
        "def test_epoch(epoch, model, loader, device):\n",
        "  ytrue = []\n",
        "  ypred = []\n",
        "  # Gọi hàm eval để model biết đang ở chế độ test \n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "      ytrue += list(labels.cpu().numpy())\n",
        "      ypred += list(predicted.cpu().numpy())\n",
        "\n",
        "  return ypred, ytrue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21jIMWPrUsA"
      },
      "source": [
        "### ***Hàm main***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   model_in = 'vgg16', 'resnet50' hoặc 'desnet': tên mạng Cnn đùng để huấn luyện\n",
        "*   PATH: đường dẫn lưu trọng số sau khi huấn luyện\n",
        "*   Đặc trưng ra = 4 vì có 4 lớp\n",
        "*   Hàm lỗi được sử dụng: Cross-Entropy\n",
        "*   Hàm tối ưu đước sử dụng: SGD - Stochastic Gradient Descent\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA-A25O0RLTG",
        "outputId": "70e66f5b-6abf-4f14-fa31-393c099e06c7"
      },
      "source": [
        "def main(PATH='./model.pth', model_in=''):\n",
        "  classes = get_clases()\n",
        "  datasets = prepare_data()\n",
        "  # img, label = datasets.train[0]\n",
        "  # plt.imshow(img)\n",
        "  # print(classes[label], img.size)\n",
        "  # print('train', len(datasets.train), 'test', len(datasets.test))\n",
        "  \n",
        "  loaders = prepare_loader(datasets)\n",
        "\n",
        "  # images, labels = iter(loaders.train).next()\n",
        "  # print(images.shape, labels.shape)\n",
        "\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  # torch.cuda.empty_cache()\n",
        "\n",
        "  # model = ResNet50().to(device)\n",
        "  # model = VGG16().to(device)\n",
        "  # images, labels = iter(loaders.train).next()\n",
        "  # outputs = model(images)\n",
        "  # print(outputs.shape)\n",
        "  # print(outputs[0])\n",
        "  # _, predicted = torch.max(outputs, dim=1)\n",
        "  # print(predicted)\n",
        "  # imshow(images, labels, predicted, classes)\n",
        "\n",
        "# Load model \n",
        "  if model_in == 'vgg16':\n",
        "    model = torchvision.models.vgg16()\n",
        "    model.classifier[6] = torch.nn.modules.linear.Linear(in_features=4096, out_features=4, bias=True)\n",
        "    # if torch.cuda.is_available():\n",
        "    model.to(device=device)\n",
        "  elif model_in == 'resnet101':\n",
        "    model = torchvision.models.resnet101(pretrained=False, progress=False)\n",
        "    model.fc = torch.nn.modules.linear.Linear(in_features=2048, out_features=4, bias=True)  \n",
        "    # if torch.cuda.is_available():\n",
        "    model.to(device=device)\n",
        "  elif model_in == 'desnet':\n",
        "    model = torchvision.models.densenet121(pretrained=False, progress=False)\n",
        "    model.classifier = torch.nn.modules.linear.Linear(in_features=1024, out_features=4, bias=True)\n",
        "    # if torch.cuda.is_available():\n",
        "    model.to(device=device)\n",
        "  else:\n",
        "    pass  \n",
        "\n",
        "  # array_accuracy = []\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "  for epoch in range(50):\n",
        "    print(f'Epoch {epoch} report: ')\n",
        "    \n",
        "    # pred_result, true_result = \n",
        "    train_epoch(epoch, model, loaders.train, loss_func, optimizer, device)\n",
        "    # print('Train report: ')\n",
        "    # print(classification_report(true_result, pred_result, target_names=classes))\n",
        "\n",
        "    print('Test report: ')\n",
        "    ypred, ytrue = test_epoch(epoch, model, loaders.test, device)\n",
        "\n",
        "    # accuracy = (ytrue_test==ypred_test).sum() / len(ytrue_test)\n",
        "    # array_accuracy.append(accuracy)\n",
        "\n",
        "    print(classification_report(ytrue, ypred, target_names=classes))\n",
        "\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "  return model\n",
        "\n",
        "model = main(PATH=\"./densenet121.pth\", model_in='desnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 report: \n",
            "Epoch 0 step 8 ave_loss 1.6465\n",
            "Epoch 0 step 17 ave_loss 1.5482\n",
            "Epoch 0 step 26 ave_loss 1.1201\n",
            "Epoch 0 step 35 ave_loss 1.1188\n",
            "Epoch 0 step 44 ave_loss 1.6656\n",
            "Epoch 0 step 53 ave_loss 1.4180\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.70      0.12      0.20        59\n",
            "       Other       0.57      0.38      0.46        78\n",
            "    PhucLong       0.16      0.45      0.23        31\n",
            "    StarBuck       0.11      0.17      0.13        30\n",
            "\n",
            "    accuracy                           0.28       198\n",
            "   macro avg       0.38      0.28      0.26       198\n",
            "weighted avg       0.47      0.28      0.30       198\n",
            "\n",
            "Epoch 1 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 step 8 ave_loss 1.4958\n",
            "Epoch 1 step 17 ave_loss 2.2915\n",
            "Epoch 1 step 26 ave_loss 1.8039\n",
            "Epoch 1 step 35 ave_loss 1.7962\n",
            "Epoch 1 step 44 ave_loss 1.3125\n",
            "Epoch 1 step 53 ave_loss 1.3917\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.00      0.00      0.00        59\n",
            "       Other       0.49      0.99      0.65        78\n",
            "    PhucLong       0.00      0.00      0.00        31\n",
            "    StarBuck       0.44      0.37      0.40        30\n",
            "\n",
            "    accuracy                           0.44       198\n",
            "   macro avg       0.23      0.34      0.26       198\n",
            "weighted avg       0.26      0.44      0.32       198\n",
            "\n",
            "Epoch 2 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 step 8 ave_loss 1.4517\n",
            "Epoch 2 step 17 ave_loss 1.3651\n",
            "Epoch 2 step 26 ave_loss 0.8204\n",
            "Epoch 2 step 35 ave_loss 1.0589\n",
            "Epoch 2 step 44 ave_loss 0.8512\n",
            "Epoch 2 step 53 ave_loss 1.2652\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.00      0.00      0.00        59\n",
            "       Other       0.47      0.51      0.49        78\n",
            "    PhucLong       0.12      0.42      0.18        31\n",
            "    StarBuck       0.00      0.00      0.00        30\n",
            "\n",
            "    accuracy                           0.27       198\n",
            "   macro avg       0.15      0.23      0.17       198\n",
            "weighted avg       0.20      0.27      0.22       198\n",
            "\n",
            "Epoch 3 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 step 8 ave_loss 0.9121\n",
            "Epoch 3 step 17 ave_loss 0.6728\n",
            "Epoch 3 step 26 ave_loss 0.8469\n",
            "Epoch 3 step 35 ave_loss 0.7804\n",
            "Epoch 3 step 44 ave_loss 1.0883\n",
            "Epoch 3 step 53 ave_loss 0.9599\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.00      0.00      0.00        59\n",
            "       Other       0.68      0.41      0.51        78\n",
            "    PhucLong       0.20      0.94      0.33        31\n",
            "    StarBuck       1.00      0.17      0.29        30\n",
            "\n",
            "    accuracy                           0.33       198\n",
            "   macro avg       0.47      0.38      0.28       198\n",
            "weighted avg       0.45      0.33      0.30       198\n",
            "\n",
            "Epoch 4 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 step 8 ave_loss 1.0276\n",
            "Epoch 4 step 17 ave_loss 0.8756\n",
            "Epoch 4 step 26 ave_loss 0.7422\n",
            "Epoch 4 step 35 ave_loss 0.7021\n",
            "Epoch 4 step 44 ave_loss 0.5647\n",
            "Epoch 4 step 53 ave_loss 0.8978\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.80      0.07      0.12        59\n",
            "       Other       0.44      1.00      0.61        78\n",
            "    PhucLong       0.00      0.00      0.00        31\n",
            "    StarBuck       0.00      0.00      0.00        30\n",
            "\n",
            "    accuracy                           0.41       198\n",
            "   macro avg       0.31      0.27      0.18       198\n",
            "weighted avg       0.41      0.41      0.28       198\n",
            "\n",
            "Epoch 5 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 step 8 ave_loss 0.9027\n",
            "Epoch 5 step 17 ave_loss 0.8588\n",
            "Epoch 5 step 26 ave_loss 0.6686\n",
            "Epoch 5 step 35 ave_loss 0.5870\n",
            "Epoch 5 step 44 ave_loss 0.9226\n",
            "Epoch 5 step 53 ave_loss 0.4574\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.96      0.73      0.83        59\n",
            "       Other       0.86      0.88      0.87        78\n",
            "    PhucLong       0.63      0.77      0.70        31\n",
            "    StarBuck       0.77      0.90      0.83        30\n",
            "\n",
            "    accuracy                           0.82       198\n",
            "   macro avg       0.81      0.82      0.81       198\n",
            "weighted avg       0.84      0.82      0.83       198\n",
            "\n",
            "Epoch 6 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 step 8 ave_loss 0.5213\n",
            "Epoch 6 step 17 ave_loss 0.5665\n",
            "Epoch 6 step 26 ave_loss 0.5522\n",
            "Epoch 6 step 35 ave_loss 0.3662\n",
            "Epoch 6 step 44 ave_loss 0.7821\n",
            "Epoch 6 step 53 ave_loss 0.5888\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.68      0.98      0.81        59\n",
            "       Other       0.88      0.88      0.88        78\n",
            "    PhucLong       0.74      0.45      0.56        31\n",
            "    StarBuck       1.00      0.53      0.70        30\n",
            "\n",
            "    accuracy                           0.79       198\n",
            "   macro avg       0.83      0.71      0.74       198\n",
            "weighted avg       0.82      0.79      0.78       198\n",
            "\n",
            "Epoch 7 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 step 8 ave_loss 0.5026\n",
            "Epoch 7 step 17 ave_loss 0.4249\n",
            "Epoch 7 step 26 ave_loss 0.3637\n",
            "Epoch 7 step 35 ave_loss 0.5258\n",
            "Epoch 7 step 44 ave_loss 1.0981\n",
            "Epoch 7 step 53 ave_loss 0.5930\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.66      0.80        59\n",
            "       Other       1.00      0.60      0.75        78\n",
            "    PhucLong       0.36      0.94      0.52        31\n",
            "    StarBuck       0.84      0.90      0.87        30\n",
            "\n",
            "    accuracy                           0.72       198\n",
            "   macro avg       0.80      0.77      0.74       198\n",
            "weighted avg       0.88      0.72      0.75       198\n",
            "\n",
            "Epoch 8 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 step 8 ave_loss 0.2845\n",
            "Epoch 8 step 17 ave_loss 0.4842\n",
            "Epoch 8 step 26 ave_loss 0.9781\n",
            "Epoch 8 step 35 ave_loss 0.6604\n",
            "Epoch 8 step 44 ave_loss 0.4570\n",
            "Epoch 8 step 53 ave_loss 0.4272\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.46      1.00      0.63        59\n",
            "       Other       0.95      0.51      0.67        78\n",
            "    PhucLong       1.00      0.06      0.12        31\n",
            "    StarBuck       0.96      0.83      0.89        30\n",
            "\n",
            "    accuracy                           0.64       198\n",
            "   macro avg       0.84      0.60      0.58       198\n",
            "weighted avg       0.81      0.64      0.60       198\n",
            "\n",
            "Epoch 9 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 step 8 ave_loss 0.2552\n",
            "Epoch 9 step 17 ave_loss 0.1872\n",
            "Epoch 9 step 26 ave_loss 0.6523\n",
            "Epoch 9 step 35 ave_loss 0.5622\n",
            "Epoch 9 step 44 ave_loss 0.3867\n",
            "Epoch 9 step 53 ave_loss 0.4879\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.02      0.03        59\n",
            "       Other       0.48      0.99      0.64        78\n",
            "    PhucLong       0.17      0.19      0.18        31\n",
            "    StarBuck       0.00      0.00      0.00        30\n",
            "\n",
            "    accuracy                           0.42       198\n",
            "   macro avg       0.41      0.30      0.21       198\n",
            "weighted avg       0.51      0.42      0.29       198\n",
            "\n",
            "Epoch 10 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 step 8 ave_loss 0.3953\n",
            "Epoch 10 step 17 ave_loss 0.4017\n",
            "Epoch 10 step 26 ave_loss 0.2584\n",
            "Epoch 10 step 35 ave_loss 0.3393\n",
            "Epoch 10 step 44 ave_loss 0.2032\n",
            "Epoch 10 step 53 ave_loss 0.2587\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.95      0.97      0.96        59\n",
            "       Other       0.96      0.91      0.93        78\n",
            "    PhucLong       0.80      0.90      0.85        31\n",
            "    StarBuck       0.97      0.93      0.95        30\n",
            "\n",
            "    accuracy                           0.93       198\n",
            "   macro avg       0.92      0.93      0.92       198\n",
            "weighted avg       0.93      0.93      0.93       198\n",
            "\n",
            "Epoch 11 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 step 8 ave_loss 0.1097\n",
            "Epoch 11 step 17 ave_loss 0.3916\n",
            "Epoch 11 step 26 ave_loss 0.3698\n",
            "Epoch 11 step 35 ave_loss 0.3271\n",
            "Epoch 11 step 44 ave_loss 0.1993\n",
            "Epoch 11 step 53 ave_loss 0.2515\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.75      1.00      0.86        59\n",
            "       Other       1.00      0.76      0.86        78\n",
            "    PhucLong       0.69      0.81      0.75        31\n",
            "    StarBuck       1.00      0.80      0.89        30\n",
            "\n",
            "    accuracy                           0.84       198\n",
            "   macro avg       0.86      0.84      0.84       198\n",
            "weighted avg       0.88      0.84      0.85       198\n",
            "\n",
            "Epoch 12 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 step 8 ave_loss 0.4467\n",
            "Epoch 12 step 17 ave_loss 0.2156\n",
            "Epoch 12 step 26 ave_loss 0.4769\n",
            "Epoch 12 step 35 ave_loss 0.3112\n",
            "Epoch 12 step 44 ave_loss 0.2577\n",
            "Epoch 12 step 53 ave_loss 0.2277\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.27      0.43        59\n",
            "       Other       0.95      0.45      0.61        78\n",
            "    PhucLong       0.22      1.00      0.36        31\n",
            "    StarBuck       1.00      0.17      0.29        30\n",
            "\n",
            "    accuracy                           0.44       198\n",
            "   macro avg       0.79      0.47      0.42       198\n",
            "weighted avg       0.86      0.44      0.47       198\n",
            "\n",
            "Epoch 13 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 step 8 ave_loss 0.2669\n",
            "Epoch 13 step 17 ave_loss 0.3123\n",
            "Epoch 13 step 26 ave_loss 0.2026\n",
            "Epoch 13 step 35 ave_loss 0.2410\n",
            "Epoch 13 step 44 ave_loss 0.3651\n",
            "Epoch 13 step 53 ave_loss 0.1498\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.40      1.00      0.58        59\n",
            "       Other       1.00      0.32      0.49        78\n",
            "    PhucLong       0.00      0.00      0.00        31\n",
            "    StarBuck       0.96      0.87      0.91        30\n",
            "\n",
            "    accuracy                           0.56       198\n",
            "   macro avg       0.59      0.55      0.49       198\n",
            "weighted avg       0.66      0.56      0.50       198\n",
            "\n",
            "Epoch 14 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 step 8 ave_loss 0.3140\n",
            "Epoch 14 step 17 ave_loss 0.2982\n",
            "Epoch 14 step 26 ave_loss 0.4476\n",
            "Epoch 14 step 35 ave_loss 0.2056\n",
            "Epoch 14 step 44 ave_loss 0.3810\n",
            "Epoch 14 step 53 ave_loss 0.1954\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.86      0.93        59\n",
            "       Other       0.93      0.91      0.92        78\n",
            "    PhucLong       0.71      0.94      0.81        31\n",
            "    StarBuck       0.93      0.93      0.93        30\n",
            "\n",
            "    accuracy                           0.90       198\n",
            "   macro avg       0.89      0.91      0.90       198\n",
            "weighted avg       0.92      0.90      0.91       198\n",
            "\n",
            "Epoch 15 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 step 8 ave_loss 0.0713\n",
            "Epoch 15 step 17 ave_loss 0.1364\n",
            "Epoch 15 step 26 ave_loss 0.2136\n",
            "Epoch 15 step 35 ave_loss 0.1955\n",
            "Epoch 15 step 44 ave_loss 0.1631\n",
            "Epoch 15 step 53 ave_loss 0.1362\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.97      0.97      0.97        59\n",
            "       Other       0.99      0.92      0.95        78\n",
            "    PhucLong       0.87      0.84      0.85        31\n",
            "    StarBuck       0.78      0.93      0.85        30\n",
            "\n",
            "    accuracy                           0.92       198\n",
            "   macro avg       0.90      0.92      0.91       198\n",
            "weighted avg       0.93      0.92      0.93       198\n",
            "\n",
            "Epoch 16 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 step 8 ave_loss 0.1088\n",
            "Epoch 16 step 17 ave_loss 0.0499\n",
            "Epoch 16 step 26 ave_loss 0.5927\n",
            "Epoch 16 step 35 ave_loss 0.5130\n",
            "Epoch 16 step 44 ave_loss 0.2942\n",
            "Epoch 16 step 53 ave_loss 0.3444\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.24      0.38        59\n",
            "       Other       0.86      0.85      0.85        78\n",
            "    PhucLong       0.28      0.97      0.44        31\n",
            "    StarBuck       1.00      0.03      0.06        30\n",
            "\n",
            "    accuracy                           0.56       198\n",
            "   macro avg       0.79      0.52      0.43       198\n",
            "weighted avg       0.83      0.56      0.53       198\n",
            "\n",
            "Epoch 17 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 step 8 ave_loss 0.1495\n",
            "Epoch 17 step 17 ave_loss 0.1344\n",
            "Epoch 17 step 26 ave_loss 0.1005\n",
            "Epoch 17 step 35 ave_loss 0.2110\n",
            "Epoch 17 step 44 ave_loss 0.2068\n",
            "Epoch 17 step 53 ave_loss 0.0770\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.95      0.97        59\n",
            "       Other       0.92      1.00      0.96        78\n",
            "    PhucLong       0.93      0.81      0.86        31\n",
            "    StarBuck       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.95       198\n",
            "   macro avg       0.95      0.93      0.94       198\n",
            "weighted avg       0.95      0.95      0.95       198\n",
            "\n",
            "Epoch 18 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 step 8 ave_loss 0.0808\n",
            "Epoch 18 step 17 ave_loss 0.1705\n",
            "Epoch 18 step 26 ave_loss 0.0430\n",
            "Epoch 18 step 35 ave_loss 0.0902\n",
            "Epoch 18 step 44 ave_loss 0.2620\n",
            "Epoch 18 step 53 ave_loss 0.2173\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.90      0.95        59\n",
            "       Other       1.00      0.63      0.77        78\n",
            "    PhucLong       0.44      1.00      0.61        31\n",
            "    StarBuck       1.00      0.87      0.93        30\n",
            "\n",
            "    accuracy                           0.80       198\n",
            "   macro avg       0.86      0.85      0.82       198\n",
            "weighted avg       0.91      0.80      0.82       198\n",
            "\n",
            "Epoch 19 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 step 8 ave_loss 0.0222\n",
            "Epoch 19 step 17 ave_loss 0.1290\n",
            "Epoch 19 step 26 ave_loss 0.0440\n",
            "Epoch 19 step 35 ave_loss 0.1983\n",
            "Epoch 19 step 44 ave_loss 0.0614\n",
            "Epoch 19 step 53 ave_loss 0.1191\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.42      0.60        59\n",
            "       Other       0.73      1.00      0.84        78\n",
            "    PhucLong       0.52      0.55      0.53        31\n",
            "    StarBuck       0.88      0.97      0.92        30\n",
            "\n",
            "    accuracy                           0.75       198\n",
            "   macro avg       0.78      0.73      0.72       198\n",
            "weighted avg       0.80      0.75      0.73       198\n",
            "\n",
            "Epoch 20 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 step 8 ave_loss 0.0556\n",
            "Epoch 20 step 17 ave_loss 0.1497\n",
            "Epoch 20 step 26 ave_loss 0.1076\n",
            "Epoch 20 step 35 ave_loss 0.0536\n",
            "Epoch 20 step 44 ave_loss 0.0881\n",
            "Epoch 20 step 53 ave_loss 0.0909\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.83      0.91        59\n",
            "       Other       0.69      1.00      0.82        78\n",
            "    PhucLong       0.78      0.23      0.35        31\n",
            "    StarBuck       1.00      0.90      0.95        30\n",
            "\n",
            "    accuracy                           0.81       198\n",
            "   macro avg       0.87      0.74      0.76       198\n",
            "weighted avg       0.84      0.81      0.79       198\n",
            "\n",
            "Epoch 21 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 step 8 ave_loss 0.1380\n",
            "Epoch 21 step 17 ave_loss 0.0685\n",
            "Epoch 21 step 26 ave_loss 0.0528\n",
            "Epoch 21 step 35 ave_loss 0.1210\n",
            "Epoch 21 step 44 ave_loss 0.0132\n",
            "Epoch 21 step 53 ave_loss 0.1775\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.86      0.32      0.47        59\n",
            "       Other       0.91      0.87      0.89        78\n",
            "    PhucLong       0.41      0.94      0.57        31\n",
            "    StarBuck       0.93      0.93      0.93        30\n",
            "\n",
            "    accuracy                           0.73       198\n",
            "   macro avg       0.78      0.77      0.71       198\n",
            "weighted avg       0.82      0.73      0.72       198\n",
            "\n",
            "Epoch 22 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 step 8 ave_loss 0.1181\n",
            "Epoch 22 step 17 ave_loss 0.0591\n",
            "Epoch 22 step 26 ave_loss 0.1107\n",
            "Epoch 22 step 35 ave_loss 0.0494\n",
            "Epoch 22 step 44 ave_loss 0.0502\n",
            "Epoch 22 step 53 ave_loss 0.1425\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.96      0.37      0.54        59\n",
            "       Other       0.73      1.00      0.84        78\n",
            "    PhucLong       0.52      0.55      0.53        31\n",
            "    StarBuck       0.80      0.93      0.86        30\n",
            "\n",
            "    accuracy                           0.73       198\n",
            "   macro avg       0.75      0.71      0.69       198\n",
            "weighted avg       0.77      0.73      0.71       198\n",
            "\n",
            "Epoch 23 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 step 8 ave_loss 0.1018\n",
            "Epoch 23 step 17 ave_loss 0.1860\n",
            "Epoch 23 step 26 ave_loss 0.1156\n",
            "Epoch 23 step 35 ave_loss 0.1233\n",
            "Epoch 23 step 44 ave_loss 0.0619\n",
            "Epoch 23 step 53 ave_loss 0.0683\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.97      0.98      0.97        59\n",
            "       Other       0.97      0.96      0.97        78\n",
            "    PhucLong       0.93      0.90      0.92        31\n",
            "    StarBuck       0.90      0.93      0.92        30\n",
            "\n",
            "    accuracy                           0.95       198\n",
            "   macro avg       0.94      0.95      0.94       198\n",
            "weighted avg       0.95      0.95      0.95       198\n",
            "\n",
            "Epoch 24 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 step 8 ave_loss 0.0312\n",
            "Epoch 24 step 17 ave_loss 0.1245\n",
            "Epoch 24 step 26 ave_loss 0.1798\n",
            "Epoch 24 step 35 ave_loss 0.1892\n",
            "Epoch 24 step 44 ave_loss 0.2254\n",
            "Epoch 24 step 53 ave_loss 0.0767\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.03      0.07        59\n",
            "       Other       0.74      0.99      0.85        78\n",
            "    PhucLong       0.48      0.84      0.61        31\n",
            "    StarBuck       0.76      0.97      0.85        30\n",
            "\n",
            "    accuracy                           0.68       198\n",
            "   macro avg       0.75      0.71      0.59       198\n",
            "weighted avg       0.78      0.68      0.58       198\n",
            "\n",
            "Epoch 25 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 step 8 ave_loss 0.1944\n",
            "Epoch 25 step 17 ave_loss 0.0542\n",
            "Epoch 25 step 26 ave_loss 0.2227\n",
            "Epoch 25 step 35 ave_loss 0.1452\n",
            "Epoch 25 step 44 ave_loss 0.0656\n",
            "Epoch 25 step 53 ave_loss 0.0680\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.88      1.00      0.94        59\n",
            "       Other       0.99      0.92      0.95        78\n",
            "    PhucLong       0.97      0.97      0.97        31\n",
            "    StarBuck       1.00      0.90      0.95        30\n",
            "\n",
            "    accuracy                           0.95       198\n",
            "   macro avg       0.96      0.95      0.95       198\n",
            "weighted avg       0.95      0.95      0.95       198\n",
            "\n",
            "Epoch 26 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 step 8 ave_loss 0.0385\n",
            "Epoch 26 step 17 ave_loss 0.0351\n",
            "Epoch 26 step 26 ave_loss 0.1809\n",
            "Epoch 26 step 35 ave_loss 0.0625\n",
            "Epoch 26 step 44 ave_loss 0.2010\n",
            "Epoch 26 step 53 ave_loss 0.1541\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.97      0.61      0.75        59\n",
            "       Other       0.82      0.97      0.89        78\n",
            "    PhucLong       0.48      0.90      0.63        31\n",
            "    StarBuck       1.00      0.33      0.50        30\n",
            "\n",
            "    accuracy                           0.76       198\n",
            "   macro avg       0.82      0.71      0.69       198\n",
            "weighted avg       0.84      0.76      0.75       198\n",
            "\n",
            "Epoch 27 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 step 8 ave_loss 0.1986\n",
            "Epoch 27 step 17 ave_loss 0.0527\n",
            "Epoch 27 step 26 ave_loss 0.0855\n",
            "Epoch 27 step 35 ave_loss 0.0539\n",
            "Epoch 27 step 44 ave_loss 0.0615\n",
            "Epoch 27 step 53 ave_loss 0.1751\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.82      0.93      0.87        59\n",
            "       Other       1.00      0.55      0.71        78\n",
            "    PhucLong       0.34      0.97      0.51        31\n",
            "    StarBuck       1.00      0.03      0.06        30\n",
            "\n",
            "    accuracy                           0.65       198\n",
            "   macro avg       0.79      0.62      0.54       198\n",
            "weighted avg       0.84      0.65      0.63       198\n",
            "\n",
            "Epoch 28 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 step 8 ave_loss 0.1949\n",
            "Epoch 28 step 17 ave_loss 0.0596\n",
            "Epoch 28 step 26 ave_loss 0.3474\n",
            "Epoch 28 step 35 ave_loss 0.2641\n",
            "Epoch 28 step 44 ave_loss 0.0629\n",
            "Epoch 28 step 53 ave_loss 0.1395\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.61      0.56      0.58        59\n",
            "       Other       0.89      0.62      0.73        78\n",
            "    PhucLong       0.86      0.19      0.32        31\n",
            "    StarBuck       0.30      0.83      0.44        30\n",
            "\n",
            "    accuracy                           0.57       198\n",
            "   macro avg       0.66      0.55      0.52       198\n",
            "weighted avg       0.71      0.57      0.58       198\n",
            "\n",
            "Epoch 29 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 step 8 ave_loss 0.2705\n",
            "Epoch 29 step 17 ave_loss 0.1583\n",
            "Epoch 29 step 26 ave_loss 0.1113\n",
            "Epoch 29 step 35 ave_loss 0.1257\n",
            "Epoch 29 step 44 ave_loss 0.0693\n",
            "Epoch 29 step 53 ave_loss 0.0663\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.97      0.98        59\n",
            "       Other       0.99      1.00      0.99        78\n",
            "    PhucLong       0.97      1.00      0.98        31\n",
            "    StarBuck       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.98      0.98      0.98       198\n",
            "weighted avg       0.99      0.98      0.98       198\n",
            "\n",
            "Epoch 30 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 step 8 ave_loss 0.1325\n",
            "Epoch 30 step 17 ave_loss 0.0178\n",
            "Epoch 30 step 26 ave_loss 0.0277\n",
            "Epoch 30 step 35 ave_loss 0.0098\n",
            "Epoch 30 step 44 ave_loss 0.0233\n",
            "Epoch 30 step 53 ave_loss 0.0942\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.97      0.66      0.79        59\n",
            "       Other       0.78      1.00      0.88        78\n",
            "    PhucLong       0.96      0.84      0.90        31\n",
            "    StarBuck       0.90      0.93      0.92        30\n",
            "\n",
            "    accuracy                           0.86       198\n",
            "   macro avg       0.91      0.86      0.87       198\n",
            "weighted avg       0.89      0.86      0.86       198\n",
            "\n",
            "Epoch 31 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 step 8 ave_loss 0.0735\n",
            "Epoch 31 step 17 ave_loss 0.0529\n",
            "Epoch 31 step 26 ave_loss 0.0195\n",
            "Epoch 31 step 35 ave_loss 0.0090\n",
            "Epoch 31 step 44 ave_loss 0.0061\n",
            "Epoch 31 step 53 ave_loss 0.0079\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.97      0.98        59\n",
            "       Other       0.97      1.00      0.99        78\n",
            "    PhucLong       0.94      0.97      0.95        31\n",
            "    StarBuck       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.98      0.98      0.98       198\n",
            "weighted avg       0.98      0.98      0.98       198\n",
            "\n",
            "Epoch 32 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 step 8 ave_loss 0.0255\n",
            "Epoch 32 step 17 ave_loss 0.0686\n",
            "Epoch 32 step 26 ave_loss 0.0530\n",
            "Epoch 32 step 35 ave_loss 0.0314\n",
            "Epoch 32 step 44 ave_loss 0.0174\n",
            "Epoch 32 step 53 ave_loss 0.0672\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.95      0.97        59\n",
            "       Other       0.99      0.99      0.99        78\n",
            "    PhucLong       0.94      1.00      0.97        31\n",
            "    StarBuck       0.94      0.97      0.95        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.97      0.98      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 33 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 step 8 ave_loss 0.0179\n",
            "Epoch 33 step 17 ave_loss 0.1129\n",
            "Epoch 33 step 26 ave_loss 0.0882\n",
            "Epoch 33 step 35 ave_loss 0.0127\n",
            "Epoch 33 step 44 ave_loss 0.0465\n",
            "Epoch 33 step 53 ave_loss 0.0216\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.93      0.97      0.95        59\n",
            "       Other       0.99      0.96      0.97        78\n",
            "    PhucLong       0.94      0.97      0.95        31\n",
            "    StarBuck       0.97      0.93      0.95        30\n",
            "\n",
            "    accuracy                           0.96       198\n",
            "   macro avg       0.96      0.96      0.96       198\n",
            "weighted avg       0.96      0.96      0.96       198\n",
            "\n",
            "Epoch 34 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 step 8 ave_loss 0.0320\n",
            "Epoch 34 step 17 ave_loss 0.1132\n",
            "Epoch 34 step 26 ave_loss 0.0156\n",
            "Epoch 34 step 35 ave_loss 0.0960\n",
            "Epoch 34 step 44 ave_loss 0.0310\n",
            "Epoch 34 step 53 ave_loss 0.0106\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.97      0.98        59\n",
            "       Other       0.94      1.00      0.97        78\n",
            "    PhucLong       0.93      0.87      0.90        31\n",
            "    StarBuck       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.96       198\n",
            "   macro avg       0.97      0.95      0.96       198\n",
            "weighted avg       0.97      0.96      0.96       198\n",
            "\n",
            "Epoch 35 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 step 8 ave_loss 0.0238\n",
            "Epoch 35 step 17 ave_loss 0.0085\n",
            "Epoch 35 step 26 ave_loss 0.0183\n",
            "Epoch 35 step 35 ave_loss 0.0073\n",
            "Epoch 35 step 44 ave_loss 0.0139\n",
            "Epoch 35 step 53 ave_loss 0.0115\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.97      0.98        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       0.94      0.94      0.94        31\n",
            "    StarBuck       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.97      0.97      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 36 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 step 8 ave_loss 0.0066\n",
            "Epoch 36 step 17 ave_loss 0.0278\n",
            "Epoch 36 step 26 ave_loss 0.0059\n",
            "Epoch 36 step 35 ave_loss 0.0237\n",
            "Epoch 36 step 44 ave_loss 0.0620\n",
            "Epoch 36 step 53 ave_loss 0.0033\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.97      0.98        59\n",
            "       Other       0.97      1.00      0.99        78\n",
            "    PhucLong       0.94      0.97      0.95        31\n",
            "    StarBuck       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.98      0.98      0.98       198\n",
            "weighted avg       0.98      0.98      0.98       198\n",
            "\n",
            "Epoch 37 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 step 8 ave_loss 0.0078\n",
            "Epoch 37 step 17 ave_loss 0.0155\n",
            "Epoch 37 step 26 ave_loss 0.0106\n",
            "Epoch 37 step 35 ave_loss 0.0043\n",
            "Epoch 37 step 44 ave_loss 0.0075\n",
            "Epoch 37 step 53 ave_loss 0.0119\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.98      1.00      0.99        59\n",
            "       Other       0.95      1.00      0.97        78\n",
            "    PhucLong       1.00      0.94      0.97        31\n",
            "    StarBuck       1.00      0.90      0.95        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.98      0.96      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 38 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 step 8 ave_loss 0.1237\n",
            "Epoch 38 step 17 ave_loss 0.0543\n",
            "Epoch 38 step 26 ave_loss 0.1186\n",
            "Epoch 38 step 35 ave_loss 0.0812\n",
            "Epoch 38 step 44 ave_loss 0.0428\n",
            "Epoch 38 step 53 ave_loss 0.0465\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.98      0.98      0.98        59\n",
            "       Other       0.94      1.00      0.97        78\n",
            "    PhucLong       1.00      0.81      0.89        31\n",
            "    StarBuck       0.90      0.93      0.92        30\n",
            "\n",
            "    accuracy                           0.95       198\n",
            "   macro avg       0.96      0.93      0.94       198\n",
            "weighted avg       0.96      0.95      0.95       198\n",
            "\n",
            "Epoch 39 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 step 8 ave_loss 0.0158\n",
            "Epoch 39 step 17 ave_loss 0.0063\n",
            "Epoch 39 step 26 ave_loss 0.0103\n",
            "Epoch 39 step 35 ave_loss 0.0049\n",
            "Epoch 39 step 44 ave_loss 0.0251\n",
            "Epoch 39 step 53 ave_loss 0.0759\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.98      0.97      0.97        59\n",
            "       Other       0.99      1.00      0.99        78\n",
            "    PhucLong       0.94      1.00      0.97        31\n",
            "    StarBuck       1.00      0.93      0.97        30\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.98      0.97      0.98       198\n",
            "weighted avg       0.98      0.98      0.98       198\n",
            "\n",
            "Epoch 40 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 step 8 ave_loss 0.0142\n",
            "Epoch 40 step 17 ave_loss 0.0341\n",
            "Epoch 40 step 26 ave_loss 0.0080\n",
            "Epoch 40 step 35 ave_loss 0.0223\n",
            "Epoch 40 step 44 ave_loss 0.0077\n",
            "Epoch 40 step 53 ave_loss 0.0044\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.98      1.00      0.99        59\n",
            "       Other       0.97      1.00      0.99        78\n",
            "    PhucLong       1.00      0.97      0.98        31\n",
            "    StarBuck       1.00      0.93      0.97        30\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.99      0.98      0.98       198\n",
            "weighted avg       0.99      0.98      0.98       198\n",
            "\n",
            "Epoch 41 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 step 8 ave_loss 0.0066\n",
            "Epoch 41 step 17 ave_loss 0.0036\n",
            "Epoch 41 step 26 ave_loss 0.0060\n",
            "Epoch 41 step 35 ave_loss 0.0084\n",
            "Epoch 41 step 44 ave_loss 0.0343\n",
            "Epoch 41 step 53 ave_loss 0.0205\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.97      0.98        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       0.97      0.94      0.95        31\n",
            "    StarBuck       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.97      0.97      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 42 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 step 8 ave_loss 0.0244\n",
            "Epoch 42 step 17 ave_loss 0.0046\n",
            "Epoch 42 step 26 ave_loss 0.0021\n",
            "Epoch 42 step 35 ave_loss 0.0032\n",
            "Epoch 42 step 44 ave_loss 0.0157\n",
            "Epoch 42 step 53 ave_loss 0.0012\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.97      0.98        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       0.97      0.94      0.95        31\n",
            "    StarBuck       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.97      0.97      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 43 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 step 8 ave_loss 0.0057\n",
            "Epoch 43 step 17 ave_loss 0.0029\n",
            "Epoch 43 step 26 ave_loss 0.0079\n",
            "Epoch 43 step 35 ave_loss 0.0023\n",
            "Epoch 43 step 44 ave_loss 0.0097\n",
            "Epoch 43 step 53 ave_loss 0.0100\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.97      0.98        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       0.97      0.94      0.95        31\n",
            "    StarBuck       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.97      0.97      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 44 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 step 8 ave_loss 0.0077\n",
            "Epoch 44 step 17 ave_loss 0.0074\n",
            "Epoch 44 step 26 ave_loss 0.0159\n",
            "Epoch 44 step 35 ave_loss 0.0026\n",
            "Epoch 44 step 44 ave_loss 0.0065\n",
            "Epoch 44 step 53 ave_loss 0.0087\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.98      0.99        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       1.00      0.94      0.97        31\n",
            "    StarBuck       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.98      0.97      0.98       198\n",
            "weighted avg       0.98      0.98      0.98       198\n",
            "\n",
            "Epoch 45 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 step 8 ave_loss 0.0023\n",
            "Epoch 45 step 17 ave_loss 0.0070\n",
            "Epoch 45 step 26 ave_loss 0.0007\n",
            "Epoch 45 step 35 ave_loss 0.0158\n",
            "Epoch 45 step 44 ave_loss 0.0027\n",
            "Epoch 45 step 53 ave_loss 0.0135\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.98      0.98      0.98        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       1.00      0.94      0.97        31\n",
            "    StarBuck       0.97      0.93      0.95        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.98      0.96      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 46 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 step 8 ave_loss 0.0102\n",
            "Epoch 46 step 17 ave_loss 0.0020\n",
            "Epoch 46 step 26 ave_loss 0.0019\n",
            "Epoch 46 step 35 ave_loss 0.0102\n",
            "Epoch 46 step 44 ave_loss 0.0020\n",
            "Epoch 46 step 53 ave_loss 0.0067\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.98      0.98      0.98        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       1.00      0.94      0.97        31\n",
            "    StarBuck       0.97      0.93      0.95        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.98      0.96      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 47 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 step 8 ave_loss 0.0076\n",
            "Epoch 47 step 17 ave_loss 0.0029\n",
            "Epoch 47 step 26 ave_loss 0.0047\n",
            "Epoch 47 step 35 ave_loss 0.0011\n",
            "Epoch 47 step 44 ave_loss 0.0028\n",
            "Epoch 47 step 53 ave_loss 0.0016\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.98      0.98      0.98        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       1.00      0.94      0.97        31\n",
            "    StarBuck       0.97      0.93      0.95        30\n",
            "\n",
            "    accuracy                           0.97       198\n",
            "   macro avg       0.98      0.96      0.97       198\n",
            "weighted avg       0.98      0.97      0.97       198\n",
            "\n",
            "Epoch 48 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 step 8 ave_loss 0.0077\n",
            "Epoch 48 step 17 ave_loss 0.0004\n",
            "Epoch 48 step 26 ave_loss 0.0006\n",
            "Epoch 48 step 35 ave_loss 0.0045\n",
            "Epoch 48 step 44 ave_loss 0.0031\n",
            "Epoch 48 step 53 ave_loss 0.0105\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       1.00      0.98      0.99        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       1.00      0.94      0.97        31\n",
            "    StarBuck       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.98      0.97      0.98       198\n",
            "weighted avg       0.98      0.98      0.98       198\n",
            "\n",
            "Epoch 49 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 step 8 ave_loss 0.0028\n",
            "Epoch 49 step 17 ave_loss 0.0018\n",
            "Epoch 49 step 26 ave_loss 0.0117\n",
            "Epoch 49 step 35 ave_loss 0.0019\n",
            "Epoch 49 step 44 ave_loss 0.0024\n",
            "Epoch 49 step 53 ave_loss 0.0049\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HighLands       0.98      1.00      0.99        59\n",
            "       Other       0.96      1.00      0.98        78\n",
            "    PhucLong       1.00      0.94      0.97        31\n",
            "    StarBuck       1.00      0.93      0.97        30\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.99      0.97      0.98       198\n",
            "weighted avg       0.98      0.98      0.98       198\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zL2AY3Lumw-"
      },
      "source": [
        "### ***Test Code ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApG6CfwdcHPu",
        "outputId": "7d0bbff9-7584-420b-8080-81e417eabc68"
      },
      "source": [
        "print(torchvision.models.densenet121())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DenseNet(\n",
            "  (features): Sequential(\n",
            "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(inplace=True)\n",
            "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (denseblock1): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition1): _Transition(\n",
            "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock2): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition2): _Transition(\n",
            "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock3): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer17): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer18): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer19): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer20): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer21): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer22): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer23): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer24): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition3): _Transition(\n",
            "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock4): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}